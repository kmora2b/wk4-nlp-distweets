{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":11595798,"sourceType":"datasetVersion","datasetId":7271706}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Disaster Tweets Classification - Mini Project\n\n## Problem and Data Overview\n\nNatural Language Processing (NLP) involves developing algorithms that allow computers to understand, interpret, and generate human language.\nThis competition challenges us to classify tweets as being related to a real disaster (1) or not (0).\nThe train.csv contains labeled examples (text + target) while test.csv contains only texts.\nTweets are short and informal, containing many misspellings, slang, and abbreviations, making this a realistic NLP task.\nUnderstanding disaster-related tweets can aid in faster emergency responses and resource allocation.","metadata":{}},{"cell_type":"markdown","source":"## Load Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport string\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T05:59:35.518207Z","iopub.execute_input":"2025-04-28T05:59:35.518568Z","iopub.status.idle":"2025-04-28T06:00:01.993509Z","shell.execute_reply.started":"2025-04-28T05:59:35.518530Z","shell.execute_reply":"2025-04-28T06:00:01.992007Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\nprint(\"Training data shape:\", train_df.shape)\nprint(\"Sample training data:\")\nprint(train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:00:56.874847Z","iopub.execute_input":"2025-04-28T06:00:56.875333Z","iopub.status.idle":"2025-04-28T06:00:57.012546Z","shell.execute_reply.started":"2025-04-28T06:00:56.875300Z","shell.execute_reply":"2025-04-28T06:00:57.011232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\n### In EDA, we plan to:\n- Analyze the distribution of the target variable to check for class imbalance.\n- Check for missing values across features.\n- Explore the text length to determine appropriate sequence padding.\n- Visualize common words in disaster vs non-disaster tweets to understand text patterns.\n\nBased on the observations from EDA, we will determine data cleaning and modeling strategies.","metadata":{}},{"cell_type":"code","source":"# Observing target distribution to check for class imbalance\nsns.countplot(data=train_df, x='target')\nplt.title('Distribution of Target Variable (0 = Not Disaster, 1 = Disaster)')\nplt.xlabel('Target')\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:01:39.771983Z","iopub.execute_input":"2025-04-28T06:01:39.772381Z","iopub.status.idle":"2025-04-28T06:01:40.064866Z","shell.execute_reply.started":"2025-04-28T06:01:39.772355Z","shell.execute_reply":"2025-04-28T06:01:40.063549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observation:\n- There is a slight imbalance (~57% non-disaster, ~43% disaster), but it is manageable.\n- Slight imbalance may still slightly bias the model towards non-disasters.","metadata":{}},{"cell_type":"code","source":"# Checking for missing values\nprint(\"Missing values in training data:\")\nprint(train_df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:01:45.697627Z","iopub.execute_input":"2025-04-28T06:01:45.698277Z","iopub.status.idle":"2025-04-28T06:01:45.709615Z","shell.execute_reply.started":"2025-04-28T06:01:45.698238Z","shell.execute_reply":"2025-04-28T06:01:45.708226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observation:\n- Location (approx 33% missing) and keyword (approx 20% missing) columns have substantial missingness.\n- Since text has no missing values and is critical for this project, we can proceed safely.","metadata":{}},{"cell_type":"code","source":"# Analyzing text lengths\ntrain_df['text_len'] = train_df['text'].apply(lambda x: len(x.split()))\nsns.histplot(train_df['text_len'], bins=30)\nplt.title('Distribution of Tweet Lengths')\nplt.xlabel('Number of Words')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:01:49.994005Z","iopub.execute_input":"2025-04-28T06:01:49.994351Z","iopub.status.idle":"2025-04-28T06:01:50.294389Z","shell.execute_reply.started":"2025-04-28T06:01:49.994325Z","shell.execute_reply":"2025-04-28T06:01:50.293418Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observation:\nMost tweets are under 30 words, reaffirming the decision to limit max sequence length.","metadata":{}},{"cell_type":"code","source":"# Generate Word Clouds to visualize common words\nfrom wordcloud import WordCloud\n\ndef plot_wordcloud(text, title):\n    wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))\n    plt.figure(figsize=(10,5))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title)\n    plt.show()\n\nplot_wordcloud(train_df[train_df['target']==1]['text'], 'Disaster Tweets')\nplot_wordcloud(train_df[train_df['target']==0]['text'], 'Non-Disaster Tweets')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:01:56.136571Z","iopub.execute_input":"2025-04-28T06:01:56.136910Z","iopub.status.idle":"2025-04-28T06:01:58.957042Z","shell.execute_reply.started":"2025-04-28T06:01:56.136885Z","shell.execute_reply":"2025-04-28T06:01:58.955832Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observation:\n- Disaster tweets highlight words like \"fire\", \"earthquake\", \"rescue\", \"evacuate\".\n- Non-disaster tweets often use disaster-related terms humorously or figuratively.","metadata":{}},{"cell_type":"markdown","source":"### Plan of Analysis based on EDA observations:\n- Since text column is complete and crucial, we'll focus our modeling on the cleaned text.\n- Given most tweets are short, we'll pad sequences to a maximum length of 100 tokens.\n- Missing keywords and location data will not be used in modeling to avoid introducing noise.\n- GloVe embeddings will be used to leverage semantic relationships between words.\n- Bidirectional LSTM will be selected to capture context from both directions in sequential data.","metadata":{}},{"cell_type":"markdown","source":"## Data Cleaning and Preprocessing\n\nCleaning text by removing noise like URLs, HTML tags, special characters, and lowercasing text.\n\n\nDecided not to remove stopwords because LSTM models benefit from preserving the sequential grammatical structure.","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+', '', text)  # remove URLs\n    text = re.sub(r'<.*?>', '', text)  # remove HTML\n    text = re.sub(r'[^a-zA-Z]', ' ', text)  # keep letters only\n    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n    text = text.strip()\n    return text\n\ntrain_df['text_clean'] = train_df['text'].apply(clean_text)\ntest_df['text_clean'] = test_df['text'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:12:03.342671Z","iopub.execute_input":"2025-04-28T06:12:03.342984Z","iopub.status.idle":"2025-04-28T06:12:03.494243Z","shell.execute_reply.started":"2025-04-28T06:12:03.342961Z","shell.execute_reply":"2025-04-28T06:12:03.493248Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observation:\nText cleaning reduces noise and ensures the model focuses on meaningful features.","metadata":{}},{"cell_type":"markdown","source":"## Word Embedding Strategy\n\n-Word embeddings convert text into a numerical format usable by neural networks.\n- TF-IDF captures word importance but ignores word order.\n- GloVe (Global Vectors) embeddings capture semantic relationships between words by using statistical information about co-occurrence.\n- This will allow the LSTM to better understand the meaning behind sequences of words.","metadata":{}},{"cell_type":"code","source":"# Tokenizing the text\nvocab_size = 20000\nmax_length = 100\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(train_df['text_clean'])\n\nX = tokenizer.texts_to_sequences(train_df['text_clean'])\nX = pad_sequences(X, maxlen=max_length)\n\nX_test = tokenizer.texts_to_sequences(test_df['text_clean'])\nX_test = pad_sequences(X_test, maxlen=max_length)\n\ny = train_df['target'].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:12:08.623951Z","iopub.execute_input":"2025-04-28T06:12:08.624352Z","iopub.status.idle":"2025-04-28T06:12:08.983260Z","shell.execute_reply.started":"2025-04-28T06:12:08.624327Z","shell.execute_reply":"2025-04-28T06:12:08.982238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:12:19.344437Z","iopub.execute_input":"2025-04-28T06:12:19.344782Z","iopub.status.idle":"2025-04-28T06:12:19.355575Z","shell.execute_reply.started":"2025-04-28T06:12:19.344758Z","shell.execute_reply":"2025-04-28T06:12:19.354257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load GloVe embeddings\nembedding_index = {}\n\nwith open('/kaggle/input/glove-6b-50d/glove.6B.50d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], dtype='float32')\n        embedding_index[word] = vector\n\nembedding_dim = 50\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in tokenizer.word_index.items():\n    if i < vocab_size:\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:24:03.425795Z","iopub.execute_input":"2025-04-28T06:24:03.426387Z","iopub.status.idle":"2025-04-28T06:24:09.714469Z","shell.execute_reply.started":"2025-04-28T06:24:03.426362Z","shell.execute_reply":"2025-04-28T06:24:09.713001Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observation:\nGloVe embeddings enable our model to have rich prior knowledge of word relationships, improving generalization.","metadata":{}},{"cell_type":"markdown","source":"## Model Architecture\n\n- Bidirectional LSTM is chosen because it processes the sequence both forward and backward.\n- This is critical for tweets, where important context might appear at the start or end of the text.\n- Dropout layers are added to prevent overfitting by randomly turning off neurons during training.","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\nmodel.add(Bidirectional(LSTM(64, return_sequences=True)))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:24:58.563617Z","iopub.execute_input":"2025-04-28T06:24:58.563965Z","iopub.status.idle":"2025-04-28T06:24:58.654405Z","shell.execute_reply.started":"2025-04-28T06:24:58.563942Z","shell.execute_reply":"2025-04-28T06:24:58.653257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observation:\nModel architecture is designed to balance complexity and regularization for robust performance.","metadata":{}},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"# EarlyStopping is used to halt training once validation performance stops improving.\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=32,\n                    validation_data=(X_val, y_val),\n                    callbacks=[early_stop])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:25:18.765870Z","iopub.execute_input":"2025-04-28T06:25:18.766305Z","iopub.status.idle":"2025-04-28T06:28:34.689134Z","shell.execute_reply.started":"2025-04-28T06:25:18.766273Z","shell.execute_reply":"2025-04-28T06:28:34.688117Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results and Evaluation","metadata":{}},{"cell_type":"code","source":"# Plotting accuracy\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:29:08.282374Z","iopub.execute_input":"2025-04-28T06:29:08.282695Z","iopub.status.idle":"2025-04-28T06:29:08.517995Z","shell.execute_reply.started":"2025-04-28T06:29:08.282672Z","shell.execute_reply":"2025-04-28T06:29:08.517015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:29:13.108040Z","iopub.execute_input":"2025-04-28T06:29:13.109235Z","iopub.status.idle":"2025-04-28T06:29:13.338628Z","shell.execute_reply.started":"2025-04-28T06:29:13.109159Z","shell.execute_reply":"2025-04-28T06:29:13.337656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation on Validation Set\nval_preds = (model.predict(X_val) > 0.5).astype('int32')\nprint(\"Validation Set Classification Report:\")\nprint(classification_report(y_val, val_preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:29:16.233925Z","iopub.execute_input":"2025-04-28T06:29:16.234301Z","iopub.status.idle":"2025-04-28T06:29:19.947388Z","shell.execute_reply.started":"2025-04-28T06:29:16.234276Z","shell.execute_reply":"2025-04-28T06:29:19.946344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observations:\n- Training and validation accuracy follow each other closely, suggesting low overfitting.\n- Validation F1-score is balanced across both classes.\n- Slight performance drop in validation is normal, indicating good generalization.","metadata":{}},{"cell_type":"markdown","source":"## Kaggle Submission","metadata":{}},{"cell_type":"code","source":"test_preds = (model.predict(X_test) > 0.5).astype('int32')\n\nsubmission = pd.DataFrame({'id': test_df['id'], 'target': test_preds.flatten()})\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\n\n# Submission file created and ready to upload to Kaggle.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T06:29:23.838313Z","iopub.execute_input":"2025-04-28T06:29:23.838639Z","iopub.status.idle":"2025-04-28T06:29:27.514238Z","shell.execute_reply.started":"2025-04-28T06:29:23.838615Z","shell.execute_reply":"2025-04-28T06:29:27.512988Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\n### Key Learnings:\n- Thorough EDA guided correct preprocessing and modeling decisions.\n- Cleaning and GloVe embeddings significantly enhanced model performance.\n- Bidirectional LSTM captured tweet sequence information effectively.\n- Regularization techniques like Dropout and EarlyStopping helped prevent overfitting.\n\n### Limitations:\n- Tweets are noisy and sometimes sarcastic or figurative, making them harder to classify.\n\n### Future Improvements:\n- Fine-tune embeddings during training.\n- Try using GloVe 100d/300d vectors for more detail.\n- Implement Transformer-based models (e.g., BERT) for better sequence modeling.\n\n## 10. References\n\n- Kaggle Disaster Tweets Competition: https://www.kaggle.com/competitions/nlp-getting-started\n- Stanford GloVe Project: https://nlp.stanford.edu/projects/glove/\n- Tensorflow/Keras API: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n- Scikit-learn Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html","metadata":{}}]}